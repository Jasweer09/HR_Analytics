{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3ac972c0-9558-413c-93cb-b3fe60abafd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1470, 35)\n",
      "Columns: ['Age', 'Attrition', 'BusinessTravel', 'DailyRate', 'Department', 'DistanceFromHome', 'Education', 'EducationField', 'EmployeeCount', 'EmployeeNumber', 'EnvironmentSatisfaction', 'Gender', 'HourlyRate', 'JobInvolvement', 'JobLevel', 'JobRole', 'JobSatisfaction', 'MaritalStatus', 'MonthlyIncome', 'MonthlyRate', 'NumCompaniesWorked', 'Over18', 'OverTime', 'PercentSalaryHike', 'PerformanceRating', 'RelationshipSatisfaction', 'StandardHours', 'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear', 'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole', 'YearsSinceLastPromotion', 'YearsWithCurrManager']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Attrition</th>\n",
       "      <th>BusinessTravel</th>\n",
       "      <th>DailyRate</th>\n",
       "      <th>Department</th>\n",
       "      <th>DistanceFromHome</th>\n",
       "      <th>Education</th>\n",
       "      <th>EducationField</th>\n",
       "      <th>EmployeeCount</th>\n",
       "      <th>EmployeeNumber</th>\n",
       "      <th>...</th>\n",
       "      <th>RelationshipSatisfaction</th>\n",
       "      <th>StandardHours</th>\n",
       "      <th>StockOptionLevel</th>\n",
       "      <th>TotalWorkingYears</th>\n",
       "      <th>TrainingTimesLastYear</th>\n",
       "      <th>WorkLifeBalance</th>\n",
       "      <th>YearsAtCompany</th>\n",
       "      <th>YearsInCurrentRole</th>\n",
       "      <th>YearsSinceLastPromotion</th>\n",
       "      <th>YearsWithCurrManager</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>1102</td>\n",
       "      <td>Sales</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>279</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>1373</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Other</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Frequently</td>\n",
       "      <td>1392</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Life Sciences</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27</td>\n",
       "      <td>No</td>\n",
       "      <td>Travel_Rarely</td>\n",
       "      <td>591</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age Attrition     BusinessTravel  DailyRate              Department  \\\n",
       "0   41       Yes      Travel_Rarely       1102                   Sales   \n",
       "1   49        No  Travel_Frequently        279  Research & Development   \n",
       "2   37       Yes      Travel_Rarely       1373  Research & Development   \n",
       "3   33        No  Travel_Frequently       1392  Research & Development   \n",
       "4   27        No      Travel_Rarely        591  Research & Development   \n",
       "\n",
       "   DistanceFromHome  Education EducationField  EmployeeCount  EmployeeNumber  \\\n",
       "0                 1          2  Life Sciences              1               1   \n",
       "1                 8          1  Life Sciences              1               2   \n",
       "2                 2          2          Other              1               4   \n",
       "3                 3          4  Life Sciences              1               5   \n",
       "4                 2          1        Medical              1               7   \n",
       "\n",
       "   ...  RelationshipSatisfaction StandardHours  StockOptionLevel  \\\n",
       "0  ...                         1            80                 0   \n",
       "1  ...                         4            80                 1   \n",
       "2  ...                         2            80                 0   \n",
       "3  ...                         3            80                 0   \n",
       "4  ...                         4            80                 1   \n",
       "\n",
       "   TotalWorkingYears  TrainingTimesLastYear WorkLifeBalance  YearsAtCompany  \\\n",
       "0                  8                      0               1               6   \n",
       "1                 10                      3               3              10   \n",
       "2                  7                      3               3               0   \n",
       "3                  8                      3               3               8   \n",
       "4                  6                      3               3               2   \n",
       "\n",
       "  YearsInCurrentRole  YearsSinceLastPromotion  YearsWithCurrManager  \n",
       "0                  4                        0                     5  \n",
       "1                  7                        1                     7  \n",
       "2                  0                        0                     0  \n",
       "3                  7                        3                     0  \n",
       "4                  2                        2                     2  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"Attrition.csv\")\n",
    "\n",
    "# Display the first few rows and column names\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c04ac1be-b2a8-4e39-a1cf-f8309e66bfcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RetentionScore sample:\n",
      "   JobSatisfaction  WorkLifeBalance  PerformanceRating  JobInvolvement  \\\n",
      "0                4                1                  3               3   \n",
      "1                2                3                  4               2   \n",
      "2                3                3                  3               2   \n",
      "3                3                3                  3               3   \n",
      "4                2                3                  3               3   \n",
      "\n",
      "   RetentionScore  \n",
      "0           0.645  \n",
      "1           0.635  \n",
      "2           0.670  \n",
      "3           0.720  \n",
      "4           0.645  \n",
      "\n",
      "Missing values:\n",
      "Age                         0\n",
      "Attrition                   0\n",
      "BusinessTravel              0\n",
      "DailyRate                   0\n",
      "Department                  0\n",
      "DistanceFromHome            0\n",
      "Education                   0\n",
      "EducationField              0\n",
      "EmployeeCount               0\n",
      "EmployeeNumber              0\n",
      "EnvironmentSatisfaction     0\n",
      "Gender                      0\n",
      "HourlyRate                  0\n",
      "JobInvolvement              0\n",
      "JobLevel                    0\n",
      "JobRole                     0\n",
      "JobSatisfaction             0\n",
      "MaritalStatus               0\n",
      "MonthlyIncome               0\n",
      "MonthlyRate                 0\n",
      "NumCompaniesWorked          0\n",
      "Over18                      0\n",
      "OverTime                    0\n",
      "PercentSalaryHike           0\n",
      "PerformanceRating           0\n",
      "RelationshipSatisfaction    0\n",
      "StandardHours               0\n",
      "StockOptionLevel            0\n",
      "TotalWorkingYears           0\n",
      "TrainingTimesLastYear       0\n",
      "WorkLifeBalance             0\n",
      "YearsAtCompany              0\n",
      "YearsInCurrentRole          0\n",
      "YearsSinceLastPromotion     0\n",
      "YearsWithCurrManager        0\n",
      "RetentionScore              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Import additional libraries\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Create a RetentionScore (proxy for retention likelihood)\n",
    "# We'll use JobSatisfaction, WorkLifeBalance, PerformanceRating, and JobInvolvement\n",
    "# Normalize each to 0-1 and take a weighted average\n",
    "df['RetentionScore'] = (\n",
    "    0.3 * (df['JobSatisfaction'] / 4) +  # Max value is 4\n",
    "    0.3 * (df['WorkLifeBalance'] / 4) +  # Max value is 4\n",
    "    0.2 * (df['PerformanceRating'] / 5) +  # Max value is 5\n",
    "    0.2 * (df['JobInvolvement'] / 4)  # Max value is 4\n",
    ")\n",
    "\n",
    "# Verify the new column\n",
    "print(\"RetentionScore sample:\")\n",
    "print(df[['JobSatisfaction', 'WorkLifeBalance', 'PerformanceRating', 'JobInvolvement', 'RetentionScore']].head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aebd59b6-1706-42ca-931e-93db79724ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating models with SMOTE...\n",
      "Logistic Regression:\n",
      "  Accuracy: 0.5054 (+/- 0.0208)\n",
      "  F1-Score: 0.2027 (+/- 0.0178)\n",
      "\n",
      "Random Forest:\n",
      "  Accuracy: 0.8354 (+/- 0.0041)\n",
      "  F1-Score: 0.0000 (+/- 0.0000)\n",
      "\n",
      "SVM:\n",
      "  Accuracy: 0.6381 (+/- 0.0163)\n",
      "  F1-Score: 0.1781 (+/- 0.0505)\n",
      "\n",
      "XGBoost:\n",
      "  Accuracy: 0.8190 (+/- 0.0142)\n",
      "  F1-Score: 0.0618 (+/- 0.0321)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Define features and target\n",
    "X_performance = df[[\"Age\", \"Gender\", \"Department\", \"JobRole\", \"MonthlyIncome\", \"YearsAtCompany\", \n",
    "                   \"OverTime\", \"JobSatisfaction\", \"WorkLifeBalance\", \"TotalWorkingYears\", \n",
    "                   \"TrainingTimesLastYear\", \"JobInvolvement\", \"EnvironmentSatisfaction\", \n",
    "                   \"RelationshipSatisfaction\"]]\n",
    "y_performance_binary = df[\"PerformanceRating\"].map({3: 0, 4: 1})\n",
    "\n",
    "# Preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), [\"Age\", \"MonthlyIncome\", \"YearsAtCompany\", \"TotalWorkingYears\", \n",
    "                                   \"TrainingTimesLastYear\", \"JobSatisfaction\", \"WorkLifeBalance\", \n",
    "                                   \"JobInvolvement\", \"EnvironmentSatisfaction\", \"RelationshipSatisfaction\"]),\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"), [\"Gender\", \"Department\", \"JobRole\", \"OverTime\"])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the models to compare\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"SVM\": SVC(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric=\"logloss\", random_state=42)  # Removed use_label_encoder\n",
    "}\n",
    "\n",
    "# Evaluate each model with SMOTE using cross-validation\n",
    "print(\"Evaluating models with SMOTE...\")\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    # Create a pipeline with SMOTE\n",
    "    pipeline = ImbPipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"smote\", SMOTE(random_state=42)),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "    \n",
    "    # Cross-validation for accuracy\n",
    "    accuracy_scores = cross_val_score(pipeline, X_performance, y_performance_binary, cv=5, scoring=\"accuracy\")\n",
    "    # Cross-validation for F1-score\n",
    "    f1_scores = cross_val_score(pipeline, X_performance, y_performance_binary, cv=5, \n",
    "                                 scoring=make_scorer(f1_score, pos_label=1))\n",
    "    \n",
    "    results[name] = {\n",
    "        \"Accuracy (Mean)\": np.mean(accuracy_scores),\n",
    "        \"Accuracy (Std)\": np.std(accuracy_scores),\n",
    "        \"F1-Score (Mean)\": np.mean(f1_scores),\n",
    "        \"F1-Score (Std)\": np.std(f1_scores)\n",
    "    }\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Accuracy: {results[name]['Accuracy (Mean)']:.4f} (+/- {results[name]['Accuracy (Std)']:.4f})\")\n",
    "    print(f\"  F1-Score: {results[name]['F1-Score (Mean)']:.4f} (+/- {results[name]['F1-Score (Std)']:.4f})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "168dd924-c256-4cb5-9d74-adfcc46ce5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Logistic Regression...\n",
      "Best parameters for Logistic Regression: {'model__C': 0.1, 'model__class_weight': None, 'model__solver': 'liblinear'}\n",
      "Best F1-Score for Logistic Regression: 0.2114\n",
      "\n",
      "Tuning SVM...\n",
      "Best parameters for SVM: {'model__C': 0.1, 'model__class_weight': None, 'model__kernel': 'linear'}\n",
      "Best F1-Score for SVM: 0.2212\n",
      "\n",
      "Best overall model: SVM\n",
      "Best F1-Score: 0.2212\n",
      "Best parameters: {'model__C': 0.1, 'model__class_weight': None, 'model__kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries for hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "# Define features and target\n",
    "X_performance = df[[\"Age\", \"Gender\", \"Department\", \"JobRole\", \"MonthlyIncome\", \"YearsAtCompany\", \n",
    "                   \"OverTime\", \"JobSatisfaction\", \"WorkLifeBalance\", \"TotalWorkingYears\", \n",
    "                   \"TrainingTimesLastYear\", \"JobInvolvement\", \"EnvironmentSatisfaction\", \n",
    "                   \"RelationshipSatisfaction\"]]\n",
    "y_performance_binary = df[\"PerformanceRating\"].map({3: 0, 4: 1})\n",
    "\n",
    "# Preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), [\"Age\", \"MonthlyIncome\", \"YearsAtCompany\", \"TotalWorkingYears\", \n",
    "                                   \"TrainingTimesLastYear\", \"JobSatisfaction\", \"WorkLifeBalance\", \n",
    "                                   \"JobInvolvement\", \"EnvironmentSatisfaction\", \"RelationshipSatisfaction\"]),\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"), [\"Gender\", \"Department\", \"JobRole\", \"OverTime\"])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Hyperparameter tuning for Logistic Regression\n",
    "print(\"Tuning Logistic Regression...\")\n",
    "logistic_pipeline = ImbPipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"smote\", SMOTE(random_state=42)),\n",
    "    (\"model\", LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "logistic_param_grid = {\n",
    "    \"model__C\": [0.1, 1, 10],\n",
    "    \"model__solver\": [\"lbfgs\", \"liblinear\"],\n",
    "    \"model__class_weight\": [None, \"balanced\"]\n",
    "}\n",
    "\n",
    "logistic_grid = GridSearchCV(\n",
    "    logistic_pipeline,\n",
    "    logistic_param_grid,\n",
    "    cv=5,\n",
    "    scoring=make_scorer(f1_score, pos_label=1),\n",
    "    n_jobs=-1\n",
    ")\n",
    "logistic_grid.fit(X_performance, y_performance_binary)\n",
    "\n",
    "print(f\"Best parameters for Logistic Regression: {logistic_grid.best_params_}\")\n",
    "print(f\"Best F1-Score for Logistic Regression: {logistic_grid.best_score_:.4f}\\n\")\n",
    "\n",
    "# Hyperparameter tuning for SVM\n",
    "print(\"Tuning SVM...\")\n",
    "svm_pipeline = ImbPipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"smote\", SMOTE(random_state=42)),\n",
    "    (\"model\", SVC(random_state=42))\n",
    "])\n",
    "\n",
    "svm_param_grid = {\n",
    "    \"model__C\": [0.1, 1, 10],\n",
    "    \"model__kernel\": [\"rbf\", \"linear\"],\n",
    "    \"model__class_weight\": [None, \"balanced\"]\n",
    "}\n",
    "\n",
    "svm_grid = GridSearchCV(\n",
    "    svm_pipeline,\n",
    "    svm_param_grid,\n",
    "    cv=5,\n",
    "    scoring=make_scorer(f1_score, pos_label=1),\n",
    "    n_jobs=-1\n",
    ")\n",
    "svm_grid.fit(X_performance, y_performance_binary)\n",
    "\n",
    "print(f\"Best parameters for SVM: {svm_grid.best_params_}\")\n",
    "print(f\"Best F1-Score for SVM: {svm_grid.best_score_:.4f}\\n\")\n",
    "\n",
    "# Compare the best models\n",
    "best_model = logistic_grid if logistic_grid.best_score_ > svm_grid.best_score_ else svm_grid\n",
    "best_model_name = \"Logistic Regression\" if logistic_grid.best_score_ > svm_grid.best_score_ else \"SVM\"\n",
    "print(f\"Best overall model: {best_model_name}\")\n",
    "print(f\"Best F1-Score: {best_model.best_score_:.4f}\")\n",
    "print(f\"Best parameters: {best_model.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "08eba006-8162-44be-9a9a-5dd18b0d6776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most important features:\n",
      "                     Feature  Importance\n",
      "8    EnvironmentSatisfaction    0.085966\n",
      "1              MonthlyIncome    0.084426\n",
      "4      TrainingTimesLastYear    0.084314\n",
      "2             YearsAtCompany    0.079245\n",
      "10               Gender_Male    0.077578\n",
      "3          TotalWorkingYears    0.074014\n",
      "9   RelationshipSatisfaction    0.073076\n",
      "0                        Age    0.070789\n",
      "21              OverTime_Yes    0.068003\n",
      "5            JobSatisfaction    0.065383\n",
      "\n",
      "Selected top 5 features:\n",
      "['EnvironmentSatisfaction' 'MonthlyIncome' 'TrainingTimesLastYear'\n",
      " 'YearsAtCompany' 'Gender_Male']\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define features and target\n",
    "X_performance = df[[\"Age\", \"Gender\", \"Department\", \"JobRole\", \"MonthlyIncome\", \"YearsAtCompany\", \n",
    "                   \"OverTime\", \"JobSatisfaction\", \"WorkLifeBalance\", \"TotalWorkingYears\", \n",
    "                   \"TrainingTimesLastYear\", \"JobInvolvement\", \"EnvironmentSatisfaction\", \n",
    "                   \"RelationshipSatisfaction\"]]\n",
    "y_performance_binary = df[\"PerformanceRating\"].map({3: 0, 4: 1})\n",
    "\n",
    "# Preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), [\"Age\", \"MonthlyIncome\", \"YearsAtCompany\", \"TotalWorkingYears\", \n",
    "                                   \"TrainingTimesLastYear\", \"JobSatisfaction\", \"WorkLifeBalance\", \n",
    "                                   \"JobInvolvement\", \"EnvironmentSatisfaction\", \"RelationshipSatisfaction\"]),\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"), [\"Gender\", \"Department\", \"JobRole\", \"OverTime\"])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a pipeline with SMOTE and Random Forest for feature importance\n",
    "feature_pipeline = ImbPipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"smote\", SMOTE(random_state=42)),\n",
    "    (\"model\", RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the pipeline\n",
    "feature_pipeline.fit(X_performance, y_performance_binary)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = feature_pipeline.named_steps[\"model\"].feature_importances_\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "num_features = [\"Age\", \"MonthlyIncome\", \"YearsAtCompany\", \"TotalWorkingYears\", \n",
    "                \"TrainingTimesLastYear\", \"JobSatisfaction\", \"WorkLifeBalance\", \n",
    "                \"JobInvolvement\", \"EnvironmentSatisfaction\", \"RelationshipSatisfaction\"]\n",
    "cat_features = feature_pipeline.named_steps[\"preprocessor\"].named_transformers_[\"cat\"].get_feature_names_out()\n",
    "all_features = np.concatenate([num_features, cat_features])\n",
    "\n",
    "# Create a DataFrame of feature importances\n",
    "importance_df = pd.DataFrame({\n",
    "    \"Feature\": all_features,\n",
    "    \"Importance\": feature_importances\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Display the top 10 features\n",
    "print(\"Top 10 most important features:\")\n",
    "print(importance_df.head(10))\n",
    "\n",
    "# Select the top 5 features\n",
    "top_features = importance_df[\"Feature\"].head(5).values\n",
    "print(\"\\nSelected top 5 features:\")\n",
    "print(top_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9b076e21-d34c-473e-8824-bb72d932c1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning XGBoost with top 5 features...\n",
      "Best parameters for XGBoost: {'model__learning_rate': 0.01, 'model__max_depth': 10, 'model__n_estimators': 100, 'model__scale_pos_weight': 10}\n",
      "Best F1-Score for XGBoost: 0.2670\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "# Define the top 5 features\n",
    "top_features = ['EnvironmentSatisfaction', 'MonthlyIncome', 'TrainingTimesLastYear', \n",
    "                'YearsAtCompany', 'Gender']\n",
    "\n",
    "# Prepare the feature set\n",
    "X_performance_top = df[top_features]\n",
    "y_performance_binary = df[\"PerformanceRating\"].map({3: 0, 4: 1})\n",
    "\n",
    "# Preprocessing\n",
    "preprocessor_top = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), ['EnvironmentSatisfaction', 'MonthlyIncome', 'TrainingTimesLastYear', 'YearsAtCompany']),\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"), ['Gender'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the pipeline with SMOTE and XGBoost\n",
    "xgb_pipeline = ImbPipeline([\n",
    "    (\"preprocessor\", preprocessor_top),\n",
    "    (\"smote\", SMOTE(random_state=42)),\n",
    "    (\"model\", XGBClassifier(eval_metric=\"logloss\", random_state=42))\n",
    "])\n",
    "\n",
    "# Define the hyperparameter grid for XGBoost\n",
    "xgb_param_grid = {\n",
    "    \"model__n_estimators\": [100, 200],\n",
    "    \"model__max_depth\": [3, 6, 10],\n",
    "    \"model__learning_rate\": [0.01, 0.1],\n",
    "    \"model__scale_pos_weight\": [1, 5, 10]  # To handle class imbalance\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "print(\"Tuning XGBoost with top 5 features...\")\n",
    "xgb_grid = GridSearchCV(\n",
    "    xgb_pipeline,\n",
    "    xgb_param_grid,\n",
    "    cv=5,\n",
    "    scoring=make_scorer(f1_score, pos_label=1),\n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb_grid.fit(X_performance_top, y_performance_binary)\n",
    "\n",
    "print(f\"Best parameters for XGBoost: {xgb_grid.best_params_}\")\n",
    "print(f\"Best F1-Score for XGBoost: {xgb_grid.best_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f36f8328-b4de-4435-affc-cf355b6623ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Voting Classifier (XGBoost + SVM)...\n",
      "Voting Classifier:\n",
      "  Accuracy: 0.2605 (+/- 0.0267)\n",
      "  F1-Score: 0.2611 (+/- 0.0041)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "# Define features for XGBoost (top 5 features)\n",
    "top_features = ['EnvironmentSatisfaction', 'MonthlyIncome', 'TrainingTimesLastYear', \n",
    "                'YearsAtCompany', 'Gender']\n",
    "X_performance_top = df[top_features]\n",
    "y_performance_binary = df[\"PerformanceRating\"].map({3: 0, 4: 1})\n",
    "\n",
    "# Preprocessing for XGBoost\n",
    "preprocessor_top = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), ['EnvironmentSatisfaction', 'MonthlyIncome', 'TrainingTimesLastYear', 'YearsAtCompany']),\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"), ['Gender'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define features for SVM (all features)\n",
    "X_performance_all = df[[\"Age\", \"Gender\", \"Department\", \"JobRole\", \"MonthlyIncome\", \"YearsAtCompany\", \n",
    "                       \"OverTime\", \"JobSatisfaction\", \"WorkLifeBalance\", \"TotalWorkingYears\", \n",
    "                       \"TrainingTimesLastYear\", \"JobInvolvement\", \"EnvironmentSatisfaction\", \n",
    "                       \"RelationshipSatisfaction\"]]\n",
    "\n",
    "# Preprocessing for SVM\n",
    "preprocessor_all = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), [\"Age\", \"MonthlyIncome\", \"YearsAtCompany\", \"TotalWorkingYears\", \n",
    "                                   \"TrainingTimesLastYear\", \"JobSatisfaction\", \"WorkLifeBalance\", \n",
    "                                   \"JobInvolvement\", \"EnvironmentSatisfaction\", \"RelationshipSatisfaction\"]),\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"), [\"Gender\", \"Department\", \"JobRole\", \"OverTime\"])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the individual models with their best parameters\n",
    "xgb_pipeline = ImbPipeline([\n",
    "    (\"preprocessor\", preprocessor_top),\n",
    "    (\"smote\", SMOTE(random_state=42)),\n",
    "    (\"model\", XGBClassifier(learning_rate=0.01, max_depth=10, n_estimators=100, \n",
    "                           scale_pos_weight=10, eval_metric=\"logloss\", random_state=42))\n",
    "])\n",
    "\n",
    "svm_pipeline = ImbPipeline([\n",
    "    (\"preprocessor\", preprocessor_all),\n",
    "    (\"smote\", SMOTE(random_state=42)),\n",
    "    (\"model\", SVC(C=0.1, class_weight=None, kernel=\"linear\", probability=True, random_state=42))\n",
    "])\n",
    "\n",
    "# Create the voting classifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        (\"xgb\", xgb_pipeline),\n",
    "        (\"svm\", svm_pipeline)\n",
    "    ],\n",
    "    voting=\"soft\"  # Use soft voting to average probabilities\n",
    ")\n",
    "\n",
    "# Evaluate the voting classifier using cross-validation\n",
    "print(\"Evaluating Voting Classifier (XGBoost + SVM)...\")\n",
    "accuracy_scores = cross_val_score(voting_clf, X_performance_all, y_performance_binary, cv=5, scoring=\"accuracy\")\n",
    "f1_scores = cross_val_score(voting_clf, X_performance_all, y_performance_binary, cv=5, \n",
    "                            scoring=make_scorer(f1_score, pos_label=1))\n",
    "\n",
    "print(f\"Voting Classifier:\")\n",
    "print(f\"  Accuracy: {np.mean(accuracy_scores):.4f} (+/- {np.std(accuracy_scores):.4f})\")\n",
    "print(f\"  F1-Score: {np.mean(f1_scores):.4f} (+/- {np.std(f1_scores):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fe89af3a-f626-46f3-917f-d8951ac94b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of engineered features:\n",
      "   EnvironmentSatisfaction  MonthlyIncome  TrainingTimesLastYear  \\\n",
      "0                        2           5993                      0   \n",
      "1                        3           5130                      3   \n",
      "2                        4           2090                      3   \n",
      "3                        4           2909                      3   \n",
      "4                        1           3468                      3   \n",
      "\n",
      "   YearsAtCompany  Gender  IncomePerYear  SatisfactionScore  TrainingPerYear  \n",
      "0               6  Female     856.142857           2.333333         0.000000  \n",
      "1              10    Male     466.363636           3.000000         0.272727  \n",
      "2               0    Male    2090.000000           3.000000         3.000000  \n",
      "3               8  Female     323.222222           3.333333         0.333333  \n",
      "4               2    Male    1156.000000           2.333333         1.000000  \n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a copy of the dataset\n",
    "df_engineered = df.copy()\n",
    "\n",
    "# Create new features\n",
    "# 1. Income per Year at Company (handle division by zero)\n",
    "df_engineered['IncomePerYear'] = df_engineered['MonthlyIncome'] / (df_engineered['YearsAtCompany'] + 1)  # Add 1 to avoid division by zero\n",
    "\n",
    "# 2. Satisfaction Score (average of satisfaction-related features)\n",
    "df_engineered['SatisfactionScore'] = (df_engineered['EnvironmentSatisfaction'] + \n",
    "                                     df_engineered['JobSatisfaction'] + \n",
    "                                     df_engineered['RelationshipSatisfaction']) / 3\n",
    "\n",
    "# 3. Training per Year (handle division by zero)\n",
    "df_engineered['TrainingPerYear'] = df_engineered['TrainingTimesLastYear'] / (df_engineered['YearsAtCompany'] + 1)\n",
    "\n",
    "# Define the feature set (top 5 features + new features)\n",
    "engineered_features = ['EnvironmentSatisfaction', 'MonthlyIncome', 'TrainingTimesLastYear', \n",
    "                      'YearsAtCompany', 'Gender', 'IncomePerYear', 'SatisfactionScore', 'TrainingPerYear']\n",
    "X_performance_engineered = df_engineered[engineered_features]\n",
    "y_performance_binary = df_engineered[\"PerformanceRating\"].map({3: 0, 4: 1})\n",
    "\n",
    "# Display the first few rows of the new features\n",
    "print(\"Sample of engineered features:\")\n",
    "print(X_performance_engineered.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f4999be5-c90e-41ca-b61e-bcd98c7d6020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning XGBoost with engineered features...\n",
      "Best parameters for XGBoost: {'model__learning_rate': 0.01, 'model__max_depth': 3, 'model__n_estimators': 100, 'model__scale_pos_weight': 10}\n",
      "Best F1-Score for XGBoost: 0.2636\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "# Define the feature set\n",
    "engineered_features = ['EnvironmentSatisfaction', 'MonthlyIncome', 'TrainingTimesLastYear', \n",
    "                      'YearsAtCompany', 'Gender', 'IncomePerYear', 'SatisfactionScore', 'TrainingPerYear']\n",
    "X_performance_engineered = df_engineered[engineered_features]\n",
    "y_performance_binary = df_engineered[\"PerformanceRating\"].map({3: 0, 4: 1})\n",
    "\n",
    "# Preprocessing\n",
    "preprocessor_engineered = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), ['EnvironmentSatisfaction', 'MonthlyIncome', 'TrainingTimesLastYear', \n",
    "                                   'YearsAtCompany', 'IncomePerYear', 'SatisfactionScore', 'TrainingPerYear']),\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"), ['Gender'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the pipeline with SMOTE and XGBoost\n",
    "xgb_pipeline = ImbPipeline([\n",
    "    (\"preprocessor\", preprocessor_engineered),\n",
    "    (\"smote\", SMOTE(random_state=42)),\n",
    "    (\"model\", XGBClassifier(eval_metric=\"logloss\", random_state=42))\n",
    "])\n",
    "\n",
    "# Define the hyperparameter grid for XGBoost\n",
    "xgb_param_grid = {\n",
    "    \"model__n_estimators\": [100, 200],\n",
    "    \"model__max_depth\": [3, 6, 10],\n",
    "    \"model__learning_rate\": [0.01, 0.1],\n",
    "    \"model__scale_pos_weight\": [1, 5, 10]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "print(\"Tuning XGBoost with engineered features...\")\n",
    "xgb_grid = GridSearchCV(\n",
    "    xgb_pipeline,\n",
    "    xgb_param_grid,\n",
    "    cv=5,\n",
    "    scoring=make_scorer(f1_score, pos_label=1),\n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb_grid.fit(X_performance_engineered, y_performance_binary)\n",
    "\n",
    "print(f\"Best parameters for XGBoost: {xgb_grid.best_params_}\")\n",
    "print(f\"Best F1-Score for XGBoost: {xgb_grid.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c0f97fcb-e289-4e57-b89c-d0d9b752b6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning XGBoost with ADASYN and top 5 features...\n",
      "Best parameters for XGBoost: {'model__learning_rate': 0.01, 'model__max_depth': 6, 'model__n_estimators': 100, 'model__scale_pos_weight': 5}\n",
      "Best F1-Score for XGBoost: 0.2647\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "# Define the top 5 features\n",
    "top_features = ['EnvironmentSatisfaction', 'MonthlyIncome', 'TrainingTimesLastYear', \n",
    "                'YearsAtCompany', 'Gender']\n",
    "X_performance_top = df[top_features]\n",
    "y_performance_binary = df[\"PerformanceRating\"].map({3: 0, 4: 1})\n",
    "\n",
    "# Preprocessing\n",
    "preprocessor_top = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), ['EnvironmentSatisfaction', 'MonthlyIncome', 'TrainingTimesLastYear', 'YearsAtCompany']),\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"), ['Gender'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the pipeline with ADASYN and XGBoost\n",
    "xgb_pipeline = ImbPipeline([\n",
    "    (\"preprocessor\", preprocessor_top),\n",
    "    (\"adasyn\", ADASYN(random_state=42)),\n",
    "    (\"model\", XGBClassifier(eval_metric=\"logloss\", random_state=42))\n",
    "])\n",
    "\n",
    "# Define the hyperparameter grid for XGBoost\n",
    "xgb_param_grid = {\n",
    "    \"model__n_estimators\": [100, 200],\n",
    "    \"model__max_depth\": [3, 6, 10],\n",
    "    \"model__learning_rate\": [0.01, 0.1],\n",
    "    \"model__scale_pos_weight\": [1, 5, 10]\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV\n",
    "print(\"Tuning XGBoost with ADASYN and top 5 features...\")\n",
    "xgb_grid = GridSearchCV(\n",
    "    xgb_pipeline,\n",
    "    xgb_param_grid,\n",
    "    cv=5,\n",
    "    scoring=make_scorer(f1_score, pos_label=1),\n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb_grid.fit(X_performance_top, y_performance_binary)\n",
    "\n",
    "print(f\"Best parameters for XGBoost: {xgb_grid.best_params_}\")\n",
    "print(f\"Best F1-Score for XGBoost: {xgb_grid.best_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "209c13a3-c3cd-4d16-bcc9-0e9f891104a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBoost model trained successfully.\n",
      "Performance model saved as 'performance_model.pkl'.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "\n",
    "# Define the top 5 features\n",
    "top_features = ['EnvironmentSatisfaction', 'MonthlyIncome', 'TrainingTimesLastYear', \n",
    "                'YearsAtCompany', 'Gender']\n",
    "X_performance_top = df[top_features]\n",
    "y_performance_binary = df[\"PerformanceRating\"].map({3: 0, 4: 1})\n",
    "\n",
    "# Preprocessing\n",
    "preprocessor_top = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), ['EnvironmentSatisfaction', 'MonthlyIncome', 'TrainingTimesLastYear', 'YearsAtCompany']),\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"), ['Gender'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the pipeline with the best XGBoost model\n",
    "performance_model = ImbPipeline([\n",
    "    (\"preprocessor\", preprocessor_top),\n",
    "    (\"smote\", SMOTE(random_state=42)),\n",
    "    (\"model\", XGBClassifier(learning_rate=0.01, max_depth=10, n_estimators=100, \n",
    "                           scale_pos_weight=10, eval_metric=\"logloss\", random_state=42))\n",
    "])\n",
    "\n",
    "# Train the model on the full dataset\n",
    "performance_model.fit(X_performance_top, y_performance_binary)\n",
    "print(\"Best XGBoost model trained successfully.\")\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(performance_model, \"performance_model.pkl\")\n",
    "print(\"Performance model saved as 'performance_model.pkl'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "adce7970-caac-4cfa-906d-037f4bd14310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attrition distribution:\n",
      "Attrition\n",
      "No     1233\n",
      "Yes     237\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the distribution of the Attrition column\n",
    "print(\"Attrition distribution:\")\n",
    "print(df[\"Attrition\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f2bf841a-550d-474b-abd7-ab78bcbf7c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RetentionScore distribution:\n",
      "RetentionScore\n",
      "0.720    207\n",
      "0.645    172\n",
      "0.795    154\n",
      "0.570    130\n",
      "0.670    100\n",
      "0.595     68\n",
      "0.745     62\n",
      "0.520     52\n",
      "0.620     48\n",
      "0.495     37\n",
      "0.760     36\n",
      "0.835     32\n",
      "0.685     31\n",
      "0.770     31\n",
      "0.870     28\n",
      "0.845     28\n",
      "0.695     27\n",
      "0.610     23\n",
      "0.710     20\n",
      "0.445     19\n",
      "0.695     18\n",
      "0.545     12\n",
      "0.635     12\n",
      "0.560     12\n",
      "0.660     10\n",
      "0.470     10\n",
      "0.820     10\n",
      "0.785     10\n",
      "0.420      9\n",
      "0.545      9\n",
      "0.535      6\n",
      "0.370      6\n",
      "0.810      6\n",
      "0.735      5\n",
      "0.510      4\n",
      "0.920      3\n",
      "0.395      3\n",
      "0.735      3\n",
      "0.885      3\n",
      "0.485      2\n",
      "0.910      2\n",
      "0.860      2\n",
      "0.585      2\n",
      "0.585      2\n",
      "0.470      1\n",
      "0.960      1\n",
      "0.460      1\n",
      "0.435      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"RetentionScore distribution:\")\n",
    "print(df[\"RetentionScore\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cf068159-2d3c-45d9-9c06-ccbe0827911c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary RetentionScore distribution (0 = Low risk, 1 = High risk):\n",
      "RetentionScore\n",
      "0    1381\n",
      "1      89\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Threshold RetentionScore to create a binary target\n",
    "y_retention_binary = (df[\"RetentionScore\"] < 0.5).astype(int)  # 1: High risk (likely to leave), 0: Low risk (likely to stay)\n",
    "\n",
    "# Check the distribution of the binary target\n",
    "print(\"Binary RetentionScore distribution (0 = Low risk, 1 = High risk):\")\n",
    "print(y_retention_binary.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e3df2ff7-6709-4ad7-8e32-f4ad7734006c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating models for retention prediction...\n",
      "Logistic Regression:\n",
      "  Accuracy: 0.9871 (+/- 0.0079)\n",
      "  F1-Score: 0.9048 (+/- 0.0569)\n",
      "\n",
      "Random Forest:\n",
      "  Accuracy: 0.9741 (+/- 0.0035)\n",
      "  F1-Score: 0.7386 (+/- 0.0545)\n",
      "\n",
      "SVM:\n",
      "  Accuracy: 0.9796 (+/- 0.0061)\n",
      "  F1-Score: 0.8336 (+/- 0.0465)\n",
      "\n",
      "XGBoost:\n",
      "  Accuracy: 0.9830 (+/- 0.0043)\n",
      "  F1-Score: 0.8614 (+/- 0.0291)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Define features and target for the retention model\n",
    "X_retention = df[[\"Age\", \"Gender\", \"Department\", \"JobRole\", \"MonthlyIncome\", \"YearsAtCompany\", \n",
    "                  \"OverTime\", \"JobSatisfaction\", \"WorkLifeBalance\", \"TotalWorkingYears\", \n",
    "                  \"TrainingTimesLastYear\", \"JobInvolvement\", \"EnvironmentSatisfaction\", \n",
    "                  \"RelationshipSatisfaction\"]]\n",
    "y_retention_binary = (df[\"RetentionScore\"] < 0.5).astype(int)  # 1: High risk, 0: Low risk\n",
    "\n",
    "# Preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), [\"Age\", \"MonthlyIncome\", \"YearsAtCompany\", \"TotalWorkingYears\", \n",
    "                                   \"TrainingTimesLastYear\", \"JobSatisfaction\", \"WorkLifeBalance\", \n",
    "                                   \"JobInvolvement\", \"EnvironmentSatisfaction\", \"RelationshipSatisfaction\"]),\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"), [\"Gender\", \"Department\", \"JobRole\", \"OverTime\"])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the models to compare\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"SVM\": SVC(random_state=42),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric=\"logloss\", random_state=42)\n",
    "}\n",
    "\n",
    "# Evaluate each model with SMOTE using cross-validation\n",
    "print(\"Evaluating models for retention prediction...\")\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    pipeline = ImbPipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"smote\", SMOTE(random_state=42)),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "    \n",
    "    # Cross-validation for accuracy\n",
    "    accuracy_scores = cross_val_score(pipeline, X_retention, y_retention_binary, cv=5, scoring=\"accuracy\")\n",
    "    # Cross-validation for F1-score\n",
    "    f1_scores = cross_val_score(pipeline, X_retention, y_retention_binary, cv=5, \n",
    "                                 scoring=make_scorer(f1_score, pos_label=1))\n",
    "    \n",
    "    results[name] = {\n",
    "        \"Accuracy (Mean)\": np.mean(accuracy_scores),\n",
    "        \"Accuracy (Std)\": np.std(accuracy_scores),\n",
    "        \"F1-Score (Mean)\": np.mean(f1_scores),\n",
    "        \"F1-Score (Std)\": np.std(f1_scores)\n",
    "    }\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Accuracy: {results[name]['Accuracy (Mean)']:.4f} (+/- {results[name]['Accuracy (Std)']:.4f})\")\n",
    "    print(f\"  F1-Score: {results[name]['F1-Score (Mean)']:.4f} (+/- {results[name]['F1-Score (Std)']:.4f})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ef02f994-7f3e-4c26-af4b-bd8497a25f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most important features for retention prediction:\n",
      "                              Feature  Importance\n",
      "5                     JobSatisfaction    0.372399\n",
      "6                     WorkLifeBalance    0.289887\n",
      "7                      JobInvolvement    0.056490\n",
      "21                       OverTime_Yes    0.039721\n",
      "10                        Gender_Male    0.027402\n",
      "3                   TotalWorkingYears    0.023825\n",
      "4               TrainingTimesLastYear    0.022626\n",
      "11  Department_Research & Development    0.017829\n",
      "0                                 Age    0.017477\n",
      "1                       MonthlyIncome    0.017267\n",
      "\n",
      "Selected top 5 features:\n",
      "['JobSatisfaction' 'WorkLifeBalance' 'JobInvolvement' 'OverTime_Yes'\n",
      " 'Gender_Male']\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define features and target\n",
    "X_retention = df[[\"Age\", \"Gender\", \"Department\", \"JobRole\", \"MonthlyIncome\", \"YearsAtCompany\", \n",
    "                  \"OverTime\", \"JobSatisfaction\", \"WorkLifeBalance\", \"TotalWorkingYears\", \n",
    "                  \"TrainingTimesLastYear\", \"JobInvolvement\", \"EnvironmentSatisfaction\", \n",
    "                  \"RelationshipSatisfaction\"]]\n",
    "y_retention_binary = (df[\"RetentionScore\"] < 0.5).astype(int)\n",
    "\n",
    "# Preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), [\"Age\", \"MonthlyIncome\", \"YearsAtCompany\", \"TotalWorkingYears\", \n",
    "                                   \"TrainingTimesLastYear\", \"JobSatisfaction\", \"WorkLifeBalance\", \n",
    "                                   \"JobInvolvement\", \"EnvironmentSatisfaction\", \"RelationshipSatisfaction\"]),\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"), [\"Gender\", \"Department\", \"JobRole\", \"OverTime\"])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a pipeline with SMOTE and Random Forest for feature importance\n",
    "feature_pipeline = ImbPipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"smote\", SMOTE(random_state=42)),\n",
    "    (\"model\", RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Fit the pipeline\n",
    "feature_pipeline.fit(X_retention, y_retention_binary)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = feature_pipeline.named_steps[\"model\"].feature_importances_\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "num_features = [\"Age\", \"MonthlyIncome\", \"YearsAtCompany\", \"TotalWorkingYears\", \n",
    "                \"TrainingTimesLastYear\", \"JobSatisfaction\", \"WorkLifeBalance\", \n",
    "                \"JobInvolvement\", \"EnvironmentSatisfaction\", \"RelationshipSatisfaction\"]\n",
    "cat_features = feature_pipeline.named_steps[\"preprocessor\"].named_transformers_[\"cat\"].get_feature_names_out()\n",
    "all_features = np.concatenate([num_features, cat_features])\n",
    "\n",
    "# Create a DataFrame of feature importances\n",
    "importance_df = pd.DataFrame({\n",
    "    \"Feature\": all_features,\n",
    "    \"Importance\": feature_importances\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Display the top 10 features\n",
    "print(\"Top 10 most important features for retention prediction:\")\n",
    "print(importance_df.head(10))\n",
    "\n",
    "# Select the top 5 features\n",
    "top_features = importance_df[\"Feature\"].head(5).values\n",
    "print(\"\\nSelected top 5 features:\")\n",
    "print(top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dcc48983-6cff-4709-a74c-6ec959584631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning Logistic Regression with top 5 features...\n",
      "Best parameters for Logistic Regression: {'model__C': 10, 'model__class_weight': None, 'model__solver': 'lbfgs'}\n",
      "Best F1-Score for Logistic Regression: 0.9476\n",
      "\n",
      "Tuning XGBoost with top 5 features...\n",
      "Best parameters for XGBoost: {'model__learning_rate': 0.1, 'model__max_depth': 6, 'model__n_estimators': 200, 'model__scale_pos_weight': 5}\n",
      "Best F1-Score for XGBoost: 0.9205\n",
      "\n",
      "Best overall model: Logistic Regression\n",
      "Best F1-Score: 0.9476\n",
      "Best parameters: {'model__C': 10, 'model__class_weight': None, 'model__solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "# Define the top 5 features (corrected OverTime_Yes to OverTime)\n",
    "top_features = ['JobSatisfaction', 'WorkLifeBalance', 'JobInvolvement', 'OverTime', 'Gender']\n",
    "X_retention_top = df[top_features]\n",
    "y_retention_binary = (df[\"RetentionScore\"] < 0.5).astype(int)\n",
    "\n",
    "# Preprocessing\n",
    "preprocessor_top = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), ['JobSatisfaction', 'WorkLifeBalance', 'JobInvolvement']),\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"), ['OverTime', 'Gender'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Hyperparameter tuning for Logistic Regression\n",
    "print(\"Tuning Logistic Regression with top 5 features...\")\n",
    "logistic_pipeline = ImbPipeline([\n",
    "    (\"preprocessor\", preprocessor_top),\n",
    "    (\"smote\", SMOTE(random_state=42)),\n",
    "    (\"model\", LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "logistic_param_grid = {\n",
    "    \"model__C\": [0.1, 1, 10],\n",
    "    \"model__solver\": [\"lbfgs\", \"liblinear\"],\n",
    "    \"model__class_weight\": [None, \"balanced\"]\n",
    "}\n",
    "\n",
    "logistic_grid = GridSearchCV(\n",
    "    logistic_pipeline,\n",
    "    logistic_param_grid,\n",
    "    cv=5,\n",
    "    scoring=make_scorer(f1_score, pos_label=1),\n",
    "    n_jobs=-1\n",
    ")\n",
    "logistic_grid.fit(X_retention_top, y_retention_binary)\n",
    "\n",
    "print(f\"Best parameters for Logistic Regression: {logistic_grid.best_params_}\")\n",
    "print(f\"Best F1-Score for Logistic Regression: {logistic_grid.best_score_:.4f}\\n\")\n",
    "\n",
    "# Hyperparameter tuning for XGBoost\n",
    "print(\"Tuning XGBoost with top 5 features...\")\n",
    "xgb_pipeline = ImbPipeline([\n",
    "    (\"preprocessor\", preprocessor_top),\n",
    "    (\"smote\", SMOTE(random_state=42)),\n",
    "    (\"model\", XGBClassifier(eval_metric=\"logloss\", random_state=42))\n",
    "])\n",
    "\n",
    "xgb_param_grid = {\n",
    "    \"model__n_estimators\": [100, 200],\n",
    "    \"model__max_depth\": [3, 6, 10],\n",
    "    \"model__learning_rate\": [0.01, 0.1],\n",
    "    \"model__scale_pos_weight\": [1, 5, 10]\n",
    "}\n",
    "\n",
    "xgb_grid = GridSearchCV(\n",
    "    xgb_pipeline,\n",
    "    xgb_param_grid,\n",
    "    cv=5,\n",
    "    scoring=make_scorer(f1_score, pos_label=1),\n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb_grid.fit(X_retention_top, y_retention_binary)\n",
    "\n",
    "print(f\"Best parameters for XGBoost: {xgb_grid.best_params_}\")\n",
    "print(f\"Best F1-Score for XGBoost: {xgb_grid.best_score_:.4f}\\n\")\n",
    "\n",
    "# Compare the best models\n",
    "best_model = logistic_grid if logistic_grid.best_score_ > xgb_grid.best_score_ else xgb_grid\n",
    "best_model_name = \"Logistic Regression\" if logistic_grid.best_score_ > xgb_grid.best_score_ else \"XGBoost\"\n",
    "print(f\"Best overall model: {best_model_name}\")\n",
    "print(f\"Best F1-Score: {best_model.best_score_:.4f}\")\n",
    "print(f\"Best parameters: {best_model.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d23f9b8f-99ba-4a66-b1be-f0bdabe7b8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Logistic Regression model trained successfully.\n",
      "Retention model saved as 'retention_model.pkl'.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "\n",
    "# Define the top 5 features\n",
    "top_features = ['JobSatisfaction', 'WorkLifeBalance', 'JobInvolvement', 'OverTime', 'Gender']\n",
    "X_retention_top = df[top_features]\n",
    "y_retention_binary = (df[\"RetentionScore\"] < 0.5).astype(int)\n",
    "\n",
    "# Preprocessing\n",
    "preprocessor_top = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), ['JobSatisfaction', 'WorkLifeBalance', 'JobInvolvement']),\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"), ['OverTime', 'Gender'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the pipeline with the best Logistic Regression model\n",
    "retention_model = ImbPipeline([\n",
    "    (\"preprocessor\", preprocessor_top),\n",
    "    (\"smote\", SMOTE(random_state=42)),\n",
    "    (\"model\", LogisticRegression(C=10, class_weight=None, solver=\"lbfgs\", max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "# Train the model on the full dataset\n",
    "retention_model.fit(X_retention_top, y_retention_binary)\n",
    "print(\"Best Logistic Regression model trained successfully.\")\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(retention_model, \"retention_model.pkl\")\n",
    "print(\"Retention model saved as 'retention_model.pkl'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "db185d00-84d3-4070-90e1-1ea6d3442afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Prediction: {'PerformanceRating': 1.0}\n",
      "Retention Prediction: {'RetentionRisk': 0.0, 'RetentionRiskProbability': 1.6062871182033433e-10}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Sample employee data\n",
    "employee_data = {\n",
    "    \"Age\": 35.0,\n",
    "    \"Gender\": \"Male\",\n",
    "    \"Department\": \"Sales\",\n",
    "    \"JobRole\": \"Sales Executive\",\n",
    "    \"MonthlyIncome\": 5000.0,\n",
    "    \"YearsAtCompany\": 5.0,\n",
    "    \"OverTime\": \"Yes\",\n",
    "    \"JobSatisfaction\": 3.0,\n",
    "    \"WorkLifeBalance\": 2.0,\n",
    "    \"TotalWorkingYears\": 10.0,\n",
    "    \"TrainingTimesLastYear\": 2.0,\n",
    "    \"JobInvolvement\": 3.0,\n",
    "    \"EnvironmentSatisfaction\": 4.0,\n",
    "    \"RelationshipSatisfaction\": 3.0\n",
    "}\n",
    "\n",
    "# Test performance prediction endpoint\n",
    "response_performance = requests.post(\"http://localhost:8001/predict_performance\", json=employee_data)\n",
    "print(\"Performance Prediction:\", response_performance.json())\n",
    "\n",
    "# Test retention prediction endpoint\n",
    "response_retention = requests.post(\"http://localhost:8001/predict_retention\", json=employee_data)\n",
    "print(\"Retention Prediction:\", response_retention.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c230f55c-3805-4b89-a943-9883163dd61f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
